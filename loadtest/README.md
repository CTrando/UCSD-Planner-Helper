# DBMS Load Test

Note: I did not optimize anything but only directly translated SQL.

## Summary

Load test the backend using different DBMS.

The output (generated dashboard) are located in this directory.

## Brief Analysis

### Intro

The number in directory names, "200-400-50-4" means there are 200 threads and the ramp up time is 400s.
Run the test for 50 loops. For each thread, it will query 4 courses from "api_data".

(ramp up time is the time when JMeter set up all the threads.
it is used to avoid the "load burst" at the beginning of the test and warm up the backend)

As observed, the number of courses queried in the http POST to "api_data" has the most significant
effects on the performance, because it is the most complicated data query.

### Disk Usage

```bash
139M	volumes/ucsd-planner-helper_sdschedule-data/
303M	volumes/ucsd-planner-helper_sdschedule-data-mongodb/
```

### Few Words about Observation

The first error happens when there are roughly 350 active users simultaneously.

Three different requests have, surprisingly (maybe not since each thread performed exactly 1 request each),
roughly the same average response time.

In brief, without (potential) optimization, MongoDB is as good as the current MariaDB.

## Test Plan Explanation

[Apache JMeter - User's Manual: Elements of a Test Plan](https://jmeter.apache.org/usermanual/test_plan.html)

Note: to simulate the real users, one can randomize the test plan.

* The test plan simulates specific number of users (number of threads), 50 by default
* And this group of users (all threads) are fully "created" after the ramp-up period, 50 by default
* The test plan is run multiple times, 5 loops by default

How it works:

* For each iteration of the loop
  * Create all users (threads) during the ramp-up period
  * For each created user (immediately after creation)
    * Wait for the timer
      * A randomized timer to simulate the "think time"
    * Initialize 2 hard-coded array of the departments and "popular" courses (const time)
      * Can be improved
    * User sends a request to get all departments
    * User sends a request to get classes of one random department
    * User sends a post request to get data of randomly chosen courses
      * The number of courses to choose is hard-coded as 4 in latest test plan
        * can be randomized
      * I did not make the courses unique since the backend does not care about duplications (also won't happen in real life (from frontend) )

## Test Environment

A clean and dedicated Debian instance.

* CPU: 1 core
* Memory: 4GB
* Network: local (not testing)

## Test Report

The report are auto generated by JMeter and are located under the "output\*" directories.
The numbers in the file name are the parameters for JMeter test plan.

The response data are also recorded in "\*.jtl" files in "output\*" directories.

A screenshot of Grafana dashboard, a backend listener I used for real time analysis, is also included.

### How to Read Test Report

Clone this branch or download the folders and open the html file in each output folder.

One can use `grep -v 200 *.jtl` to read the errors of all response data.

(Another possible information to analysis is the output from the container. To record it in the future.)

## How to Run the Test

### Note about randomness (disabled in latest test plan)

To simulate real users, I added randomness to the test plans.

The most significant factor that affects the performance is the number of classes queried in "get_data" api.

It can be changed in the pre-processor of the http request sampler.

One can either get very lucky (all users only query 1 class), or very unlucky (all users query 10 classes).

Therefore, longer test means more big number_of_classes were chosen. And there will be more errors.

### Switch Database

To switch database, change the following 3 files:

* docker-compose.yml
  * comment out the current database service
  * uncomment the database service to use
  * and the volumes
  * Why? To keep resources available the same
* backend\application.py
  * change the import at the beginning
* backend\datautil\webreg_scrape_upload.py
  * change the export call at the end

### Reset Everything (optional)

To avoid any possible inconsistency, it is recommended to delete everything and start over.

Prune any docker cache and scrape everything from the beginning.

Don't forget the volumes.

### Run Load Test

To run the test, install "JMeter" and run the jmeter test plan file located under the script folder.

Note the output directories are specified in the command.

* The output log file must be non-exist or empty
* The report directory must be non-exist and can be created or exist and empty

The cli parameters for this test plan are listed in "SDSDBTproperties.txt"

I only have a script to run the plan on Windows. You can use that file as reference.

Note the environment variables in the script.

Also I used a Grafana backend listener to see the real time data. Disable it in the test plan if you don't want to use it.

To use it, you need an "InfluxDB" instance and a "Grafana server" (also a local instance).
You will need to change the ports and create a database in InfluxDB via `curl`, and some config.